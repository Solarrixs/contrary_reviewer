# Cerebras Draft 1

## **Thesis**

Since the release of Google’s [Transformer model](https://arxiv.org/pdf/1706.03762) in 2017, which introduced a new method for artificial intelligence development, AI technologies have advanced rapidly. From 2018 to 2020, parameter counts in emerging AI models increased [1,000-fold](https://youtu.be/8i1_Ru5siXc?t=85), resulting in an average [4.1-fold](https://epochai.org/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year) increase in training compute use every year as larger, more compute-intensive models were released. Among frontier models, the increase in training compute is even greater. OpenAI’s [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), released in [November 2019](https://openai.com/index/gpt-2-1-5b-release/), was trained with [4.3 million](https://epochai.org/data/epochdb/table) petaFLOPS of compute. Less than six months later, [GPT-3](https://arxiv.org/pdf/2005.14165) was developed using [314 million](https://arxiv.org/pdf/2005.14165) petaFLOPS of training compute. By 2023, [GPT-4](https://arxiv.org/abs/2303.08774) required [21 billion](https://posts.voronoiapp.com/technology/Googles-Gemini-Ultra-Cost-191M-to-Develop--1088) petaFLOPS of training compute, while Google’s release of [Gemini Ultra](https://arxiv.org/abs/2312.11805) in late 2023 required [50 billion](https://aiindex.stanford.edu/wp-content/uploads/2024/05/HAI_AI-Index-Report-2024.pdf) petaFLOPS of training compute. Over four years, frontier models have increased their annual training compute usage by 2.9K times.